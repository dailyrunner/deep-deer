"""LangChain-based integration pipeline"""
import logging
from typing import List, Dict, Any, Optional
from langchain_community.llms import Ollama
from app.services.embeddings import embedding_service
from langchain.chains import LLMChain, RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate

from app.core.config import settings
from app.services.vector_store import vector_manager
from app.services.nlq_processor import nlq_processor

logger = logging.getLogger(__name__)


class LangChainPipeline:
    """Main LangChain integration pipeline for Deep Deer"""

    def __init__(self):
        self.llm = None
        self.embeddings = None
        self.is_ready = False

    async def initialize(self):
        """Initialize LangChain components"""
        try:
            from app.services.llm_provider import llm_service

            # Try to use llm_service if it's initialized
            if llm_service.is_ready and llm_service.default_provider:
                provider = llm_service.providers.get(llm_service.default_provider)

                # Use the provider's LLM if available
                if provider and hasattr(provider, 'llm') and provider.llm:
                    self.llm = provider.llm
                    logger.info(f"Using {llm_service.default_provider} provider for LangChain pipeline")
                else:
                    # Fallback to Ollama
                    self.llm = Ollama(
                        base_url=settings.ollama_base_url,
                        model=settings.ollama_model,
                        temperature=0.7,
                        num_predict=2000
                    )
                    logger.info("Using Ollama as fallback for LangChain pipeline")
            else:
                # Default to Ollama if llm_service is not ready
                self.llm = Ollama(
                    base_url=settings.ollama_base_url,
                    model=settings.ollama_model,
                    temperature=0.7,
                    num_predict=2000
                )
                logger.info("Using default Ollama for LangChain pipeline")

            # Initialize embeddings
            embedding_service.initialize()
            self.embeddings = embedding_service.get_embeddings()

            self.is_ready = True
            logger.info("LangChain pipeline initialized")
        except Exception as e:
            logger.error(f"Failed to initialize LangChain pipeline: {e}")
            self.is_ready = False

    def create_sql_chain(self):
        """Create a chain for SQL generation"""
        sql_prompt = PromptTemplate(
            input_variables=["schema", "question", "examples"],
            template="""You are an expert SQL query generator.

Database Schema:
{schema}

Examples:
{examples}

Question: {question}

Generate a valid SQL query to answer the question. Return only the SQL query without any explanation.

SQL Query:"""
        )

        return LLMChain(llm=self.llm, prompt=sql_prompt)

    def create_idea_generation_chain(self):
        """Create a chain for idea generation"""
        idea_prompt = PromptTemplate(
            input_variables=["sql_data", "document_data", "web_data", "requirements"],
            template="""ë‹¹ì‹ ì€ ë°ì´í„° ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëžµ ì»¨ì„¤í„´íŠ¸ìž…ë‹ˆë‹¤.
ê¸°ì—…ì˜ ë‚´ë¶€ ë°ì´í„°ì™€ ì‹œìž¥ íŠ¸ë Œë“œë¥¼ ì¢…í•©í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

**ì¤‘ìš”: ëª¨ë“  ì‘ë‹µì€ í•œêµ­ì–´ë¡œ ìž‘ì„±í•´ì£¼ì„¸ìš”.**

## ìž…ë ¥ ì •ë³´

**ì‚¬ìš©ìž ìš”ì²­**: {requirements}

**ðŸ“Š ë‚´ë¶€ DB ë°ì´í„° (NLQ ê²°ê³¼)**:
{sql_data}

**ðŸ“ ì‚¬ë‚´ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ (Vector DB)**:
{document_data}

**ðŸŒ ì™¸ë¶€ ì‹œìž¥ íŠ¸ë Œë“œ (ì›¹ ê²€ìƒ‰)**:
{web_data}

## ìš”êµ¬ì‚¬í•­

ë‹¤ìŒ êµ¬ì¡°ë¡œ **3ê°œì˜ êµ¬ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì•„ì´ë””ì–´**ë¥¼ í•œêµ­ì–´ë¡œ ì œì‹œí•˜ì„¸ìš”:

### ðŸ’¡ ì•„ì´ë””ì–´ 1: [ì•„ì´ë””ì–´ëª…]

**ðŸ“Š ë°ì´í„° ê·¼ê±°**
- DB ë°ì´í„° ë¶„ì„ ê²°ê³¼: [SQL ê²°ê³¼ì—ì„œ ë°œê²¬ëœ êµ¬ì²´ì  íŒ¨í„´]
- ì‚¬ë‚´ ë¬¸ì„œ ì¸ì‚¬ì´íŠ¸: [ë¬¸ì„œì—ì„œ ë°œê²¬ëœ ê¸°ì¡´ ì—­ëŸ‰/ê²½í—˜]
- ì‹œìž¥ íŠ¸ë Œë“œ ì—°ê²°: [ì›¹ ê²€ìƒ‰ ê²°ê³¼ì™€ì˜ ì—°ê²°ì ]

**ðŸŽ¯ íƒ€ê²Ÿ ê³ ê° & ì‹œìž¥**
- êµ¬ì²´ì ì¸ íƒ€ê²Ÿ ê³ ê°êµ°
- ì‹œìž¥ ê·œëª¨ ë° ì„±ìž¥ ê°€ëŠ¥ì„±

**âš™ï¸ êµ¬í˜„ ë°©ì•ˆ**
- ê¸°ìˆ ì  ì ‘ê·¼ ë°©ë²•
- í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ë° ì¸í”„ë¼
- ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íš (3ë‹¨ê³„)

**ðŸ’° ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸**
- ìˆ˜ìµ êµ¬ì¡°
- ì˜ˆìƒ ROI ë° íƒ€ìž„ë¼ì¸

**ðŸš§ ë¦¬ìŠ¤í¬ & ì™„í™”ë°©ì•ˆ**
- ì£¼ìš” ìœ„í—˜ ìš”ì†Œ 2ê°€ì§€
- ê°ê°ì˜ ëŒ€ì‘ ì „ëžµ

### ðŸ’¡ ì•„ì´ë””ì–´ 2: [ì•„ì´ë””ì–´ëª…]
[ë™ì¼í•œ êµ¬ì¡°ë¡œ ë°˜ë³µ]

### ðŸ’¡ ì•„ì´ë””ì–´ 3: [ì•„ì´ë””ì–´ëª…]
[ë™ì¼í•œ êµ¬ì¡°ë¡œ ë°˜ë³µ]

## ðŸ“ˆ ì¢…í•© ìš°ì„ ìˆœìœ„ ë° ê¶Œìž¥ì‚¬í•­
1. [ì•„ì´ë””ì–´ëª…] - ì‹¤í–‰ ìš©ì´ì„±: [ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ], ìˆ˜ìµì„±: [ë†’ìŒ/ì¤‘ìƒ/ì¤‘ê°„/ë‚®ìŒ]
2. [ì•„ì´ë””ì–´ëª…] - ì‹¤í–‰ ìš©ì´ì„±: [ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ], ìˆ˜ìµì„±: [ë†’ìŒ/ì¤‘ìƒ/ì¤‘ê°„/ë‚®ìŒ]
3. [ì•„ì´ë””ì–´ëª…] - ì‹¤í–‰ ìš©ì´ì„±: [ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ], ìˆ˜ìµì„±: [ë†’ìŒ/ì¤‘ìƒ/ì¤‘ê°„/ë‚®ìŒ]

**ì²« ë²ˆì§¸ ì¶”ì§„ ê¶Œìž¥**: [ì´ìœ ì™€ í•¨ê»˜]

ë¹„ì¦ˆë‹ˆìŠ¤ ì•„ì´ë””ì–´:"""
        )

        return LLMChain(llm=self.llm, prompt=idea_prompt)

    def create_rag_chain(self, retriever):
        """Create a RAG chain for question answering"""
        # Create prompt template
        system_prompt = """You are an intelligent assistant helping users find insights from their data.
        Use the following context to answer the question. If you don't know the answer, say so.

Context:
{context}

Question: {input}

Answer:"""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt)
        ])

        # Create document chain
        document_chain = create_stuff_documents_chain(self.llm, prompt)

        # Create retrieval chain
        retrieval_chain = create_retrieval_chain(retriever, document_chain)

        return retrieval_chain

    async def process_idea_request(
        self,
        user_request: str,
        use_web_search: bool = True
    ) -> Dict[str, Any]:
        """Process a complete idea generation request"""
        if not self.is_ready:
            raise RuntimeError("LangChain pipeline is not ready")

        results = {
            "request": user_request,
            "sql_data": None,
            "document_context": None,
            "web_insights": None,
            "generated_ideas": None,
            "error": None
        }

        try:
            import asyncio

            # Create tasks for parallel execution
            tasks = []

            # Task 1: Extract relevant data from database using NLQ
            logger.info("Starting parallel data collection...")

            async def get_sql_data():
                try:
                    nlq_result = await nlq_processor.process_query(user_request)
                    if nlq_result.success and nlq_result.data:
                        return {
                            "query": nlq_result.sql,
                            "data": nlq_result.data[:10],  # Limit to 10 rows
                            "tables": nlq_result.tables_used
                        }
                except Exception as e:
                    logger.error(f"Error in NLQ processing: {e}")
                return None

            # Task 2: Search relevant documents from vector store
            async def get_document_context():
                try:
                    doc_results = await vector_manager.search(
                        query=user_request,
                        k=5
                    )
                    if doc_results:
                        return [
                            {
                                "content": doc.page_content[:500],
                                "metadata": doc.metadata
                            }
                            for doc in doc_results
                        ]
                except Exception as e:
                    logger.error(f"Error in vector search: {e}")
                return None

            # Task 3: Web search for market trends (if enabled)
            async def get_web_insights():
                if not use_web_search:
                    return None
                try:
                    from app.services.web_search import web_searcher
                    # Extract keywords from user request first
                    keyword_prompt = PromptTemplate(
                        input_variables=["request"],
                        template="""ê¸°ì—… ë‚´ë¶€ ë°ì´í„° ê¸°ë°˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì•„ì´ë””ì–´ ë°œêµ´ì„ ìœ„í•œ ì‹œìž¥ íŠ¸ë Œë“œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ìž ìš”ì²­: {request}

ë‹¤ìŒ ê´€ì ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
1. **ê¸°ìˆ  íŠ¸ë Œë“œ**: ê´€ë ¨ ì‹ ê¸°ìˆ , AI, ë””ì§€í„¸ íŠ¸ëžœìŠ¤í¬ë©”ì´ì…˜
2. **ì‹œìž¥ ë™í–¥**: ì‚°ì—… ë™í–¥, ê²½ìŸì‚¬ ë¶„ì„, ì†Œë¹„ìž íŠ¸ë Œë“œ
3. **ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸**: ìƒˆë¡œìš´ ìˆ˜ìµ ëª¨ë¸, ì„œë¹„ìŠ¤ ëª¨ë¸
4. **ê·œì œ/ì •ì±…**: ê´€ë ¨ ê·œì œ ë³€í™”, ì •ë¶€ ì •ì±…

ì¤‘ìš”:
- ê°€ìž¥ í•µì‹¬ì ì¸ í‚¤ì›Œë“œ 3-5ê°œë§Œ ì„ íƒ
- ê° í‚¤ì›Œë“œëŠ” 2-3ë‹¨ì–´ ì´ë‚´
- íšŒì‚¬ëª…ì´ë‚˜ êµ¬ì²´ì ì¸ ë‚´ë¶€ ì •ë³´ëŠ” ì œì™¸

í‚¤ì›Œë“œ (ì˜ë¬¸, ì‰¼í‘œë¡œ êµ¬ë¶„, ìµœëŒ€ 5ê°œ):"""
                    )

                    keyword_chain = LLMChain(llm=self.llm, prompt=keyword_prompt)
                    keyword_result = await keyword_chain.ainvoke({"request": user_request})
                    search_keywords = keyword_result["text"].strip()

                    # Limit keywords to prevent URI too long error
                    keywords_list = [k.strip() for k in search_keywords.split(',')][:5]  # Max 5 keywords
                    search_keywords = ', '.join(keywords_list)

                    # Further limit total length
                    if len(search_keywords) > 200:
                        # Take only first 3 keywords if still too long
                        keywords_list = keywords_list[:3]
                        search_keywords = ', '.join(keywords_list)

                    logger.info(f"Extracted search keywords: {search_keywords}")

                    web_results = await web_searcher.search_and_summarize(
                        query=search_keywords,
                        context="",  # Will be updated with actual context later
                        max_results=5
                    )
                    return web_results
                except Exception as e:
                    logger.error(f"Error in web search: {e}")
                return None

            # Execute all tasks in parallel
            sql_data, document_context, web_insights = await asyncio.gather(
                get_sql_data(),
                get_document_context(),
                get_web_insights()
            )

            # Store results
            results["sql_data"] = sql_data
            results["document_context"] = document_context
            results["web_insights"] = web_insights

            # Step 4: Generate ideas using all collected data
            logger.info("Step 4: Generating business ideas")
            idea_chain = self.create_idea_generation_chain()

            # Prepare structured data for each source
            sql_data_formatted = self._format_sql_data(results.get("sql_data"))
            document_data_formatted = self._format_document_data(results.get("document_context"))
            web_data_formatted = self._format_web_data(results.get("web_insights"))

            generated = await idea_chain.ainvoke({
                "sql_data": sql_data_formatted,
                "document_data": document_data_formatted,
                "web_data": web_data_formatted,
                "requirements": user_request
            })

            results["generated_ideas"] = generated["text"]

        except Exception as e:
            logger.error(f"Error in idea generation pipeline: {e}")
            results["error"] = str(e)

        return results

    async def _create_safe_web_query(self, original_query: str, current_results: Dict) -> str:
        """Create a safe web search query by extracting key concepts and removing sensitive information"""
        # Extract general concepts without specific internal data
        filter_prompt = PromptTemplate(
            input_variables=["query", "context"],
            template="""Convert the following query into an optimized web search query.
Extract the main keywords and concepts for effective web search.
Remove any company-specific names, internal data, or sensitive information.
Focus on industry terms, technologies, and general business concepts.

Original Query: {query}
Internal Context Summary: {context}

Output a concise search query with 2-5 key terms that will return relevant market insights.
Web Search Query:"""
        )

        chain = LLMChain(llm=self.llm, prompt=filter_prompt)

        context_summary = self._summarize_internal_context(current_results)[:500]

        result = await chain.ainvoke({
            "query": original_query,
            "context": context_summary
        })

        return result["text"].strip()

    def _summarize_internal_context(self, results: Dict) -> str:
        """Summarize internal context from results"""
        context_parts = []

        if results.get("sql_data") and results["sql_data"]["data"]:
            context_parts.append(f"Database contains {len(results['sql_data']['data'])} relevant records")

        if results.get("document_context"):
            context_parts.append(f"Found {len(results['document_context'])} relevant documents")

        return ". ".join(context_parts) if context_parts else "No internal context available"

    def _prepare_context_for_ideas(self, results: Dict) -> str:
        """Prepare comprehensive context for idea generation"""
        context_parts = []

        # Add SQL data context
        if results.get("sql_data") and results["sql_data"]["data"]:
            context_parts.append("=== Database Insights ===")
            for row in results["sql_data"]["data"][:5]:  # Top 5 rows
                context_parts.append(str(row))

        # Add document context
        if results.get("document_context"):
            context_parts.append("\n=== Document Insights ===")
            for doc in results["document_context"][:3]:  # Top 3 documents
                context_parts.append(doc["content"])

        return "\n\n".join(context_parts) if context_parts else "Limited internal data available"

    def _format_sql_data(self, sql_data: dict) -> str:
        """Format SQL data for idea generation prompt"""
        if not sql_data or not sql_data.get("data"):
            return "ë‚´ë¶€ DB ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        formatted = f"ì‹¤í–‰ëœ ì¿¼ë¦¬: {sql_data.get('query', 'N/A')}\n"
        formatted += f"ì‚¬ìš©ëœ í…Œì´ë¸”: {', '.join(sql_data.get('tables', []))}\n\n"
        formatted += "ì£¼ìš” ë°ì´í„° íŒ¨í„´:\n"

        data = sql_data.get("data", [])[:5]  # ìƒìœ„ 5ê°œ í–‰ë§Œ í‘œì‹œ
        for i, row in enumerate(data, 1):
            formatted += f"{i}. "
            row_items = []
            for key, value in row.items():
                row_items.append(f"{key}: {value}")
            formatted += ", ".join(row_items) + "\n"

        if len(sql_data.get("data", [])) > 5:
            formatted += f"... ì´ {len(sql_data.get('data', []))}ê°œ ë ˆì½”ë“œ ì¤‘ ìƒìœ„ 5ê°œë§Œ í‘œì‹œ"

        return formatted

    def _format_document_data(self, document_data: list) -> str:
        """Format document data for idea generation prompt"""
        if not document_data:
            return "ê²€ìƒ‰ëœ ì‚¬ë‚´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        formatted = "ê´€ë ¨ ì‚¬ë‚´ ë¬¸ì„œ ìš”ì•½:\n\n"
        for i, doc in enumerate(document_data[:3], 1):  # ìƒìœ„ 3ê°œ ë¬¸ì„œë§Œ
            formatted += f"{i}. ë¬¸ì„œ ë‚´ìš©: {doc.get('content', '')[:200]}...\n"
            metadata = doc.get('metadata', {})
            if 'source' in metadata:
                formatted += f"   ì¶œì²˜: {metadata['source']}\n"
            formatted += "\n"

        return formatted

    def _format_web_data(self, web_data: dict) -> str:
        """Format web search data for idea generation prompt"""
        if not web_data:
            return "ì™¸ë¶€ ì‹œìž¥ íŠ¸ë Œë“œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

        formatted = f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {web_data.get('query', 'N/A')}\n\n"

        # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        results = web_data.get('results', [])
        if results:
            formatted += "ì£¼ìš” ê²€ìƒ‰ ê²°ê³¼:\n"
            for i, result in enumerate(results[:3], 1):  # ìƒìœ„ 3ê°œ ê²°ê³¼
                formatted += f"{i}. {result.get('title', 'ì œëª© ì—†ìŒ')}\n"
                formatted += f"   ì¶œì²˜: {result.get('source', 'ì¶œì²˜ ë¶ˆëª…')}\n"
                formatted += f"   ë‚´ìš©: {result.get('snippet', '')[:150]}...\n\n"

        # AI ìš”ì•½ ì •ë³´
        if 'summary' in web_data:
            formatted += "ì‹œìž¥ íŠ¸ë Œë“œ ìš”ì•½:\n"
            formatted += web_data['summary'][:500] + "..."

        return formatted

    async def answer_question(self, question: str, store_names: Optional[List[str]] = None) -> str:
        """Answer a question using RAG"""
        if not self.is_ready:
            raise RuntimeError("LangChain pipeline is not ready")

        # Get relevant documents
        docs = await vector_manager.search(
            query=question,
            store_names=store_names,
            k=5
        )

        if not docs:
            # Try to answer using SQL
            nlq_result = await nlq_processor.process_query(question)
            if nlq_result.success and nlq_result.data:
                return self._format_sql_answer(nlq_result)
            return "I couldn't find relevant information to answer your question."

        # Create a simple QA chain
        qa_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""Based on the following context, answer the question.
If the answer is not in the context, say "I don't have enough information to answer this question."

Context:
{context}

Question: {question}

Answer:"""
        )

        qa_chain = LLMChain(llm=self.llm, prompt=qa_prompt)

        # Prepare context from documents
        context = "\n\n".join([doc.page_content for doc in docs])

        result = await qa_chain.ainvoke({
            "context": context,
            "question": question
        })

        return result["text"]

    def _format_sql_answer(self, nlq_result) -> str:
        """Format SQL query results as an answer"""
        if not nlq_result.data:
            return "The query returned no results."

        # Format first few rows as answer
        answer_parts = [f"Found {len(nlq_result.data)} results:"]

        for row in nlq_result.data[:5]:  # Show first 5 rows
            row_str = ", ".join([f"{k}: {v}" for k, v in row.items()])
            answer_parts.append(f"- {row_str}")

        if len(nlq_result.data) > 5:
            answer_parts.append(f"... and {len(nlq_result.data) - 5} more results")

        return "\n".join(answer_parts)


# Global instance
langchain_pipeline = LangChainPipeline()